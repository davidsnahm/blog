{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its most basic form, logistic regression is a binary classification algorithm that classifies data into two groups. For example, a common use case of logistic regression could be predicting whether a customer will buy a car (class 1) or not (class 0). \n",
    "\n",
    "Logistic regression takes a datapoint **x** as input and outputs the predicted probability that $x$ belongs to class 1. Similar to linear regression, we are trying to find the best weights $w$ to weigh the features of the data. However, if we simply take a weighted sum of the features ($w^{\\top}x$), this would not be a probability (because it could be any real number, not just limited to the range [0, 1]). Therefore, we want to \"squash\" $w^{\\top}x$ into a number between 0 and 1. We do this by using the **sigmoid** function: $$ f(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of the sigmoid function is [0, 1] which we can see by looking at the value of the sigmoid at different values: $$ \\lim_{x \\to -\\infty} f(x) = 0 \\\\ \\lim_{x \\to \\infty} f(x) = 1 \\\\ f(0) = 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the goal of logistic regression (binary classification) and how logistic regression can take in data and output probabilities (sigmoid function), but we still need to understand how logistic regression finds the best weights $w$. In linear regression, we learned the best weights $w$ by using least squares. However, least squares would not be a good method to use for logistic regression for several reasons. \n",
    "\n",
    "1) Least squares would heavily penalize outliers even if those outliers should not be affecting the decision boundary very much. \n",
    "\n",
    "2) Least squares has a probabilistic justification for being used to optimize linear regression. Least squares is equivalent to the MLE under the Gaussian noise assumption. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([-5, -4, -3, -2, -1, 1, 2, 3, 15])\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we instead minimize cross entropy loss (or log loss) to calculate the best weights for logistic regression. We cannot simply take the gradient and come up with a closed form solution as we did in the linear regression case because we no longer end up with a linear system of equations when we set the gradient equal to zero. We instead use **gradient descent** or **stochastic gradient descent** to minimize the cross entropy loss. By performing gradient descent using the gradients of the cross entropy loss with respect to the logistic regression weights, we can find the weights that minimize the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros/Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "* Easy to train\n",
    "* Gives interpretable output (probability of class 1)\n",
    "\n",
    "Cons:\n",
    "* Linear decision boundary can lead to underfitting\n",
    "* Would need to choose correct feature transformation to form non-linear decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
